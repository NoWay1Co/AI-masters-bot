# Как создавался бот-консультант для абитуриентов ИТМО

## Обзор задачи

Передо мной стояла задача создать телеграм-бота, который поможет абитуриентам разобраться с магистерскими программами ИТМО по ИИ. Нужно было реализовать парсинг данных с сайтов, диалоговую систему для ответов на вопросы и персональные рекомендации по выборочным дисциплинам.

Конкретно требовалось:
1. Парсить учебные планы с двух страниц программ
2. Создать диалоговую систему на базе telegram-бота  
3. Генерировать рекомендации по выборочным дисциплинам
4. Отвечать только на релевантные вопросы по обучению

## Выбор технологического стека

### Основные технологии

**Python 3.9+**

**aiogram 3.13.0** - современная асинхронная библиотека для работы с Telegram Bot API. Выбрал именно третью версию из-за улучшенной архитектуры с роутерами и FSM, что делает код более структурированным.

**Ollama + локальная LLM** - принял решение использовать локальную модель вместо API по нескольким причинам:
- Полный контроль над данными
- Отсутствие затрат на API
- Гибкость в настройке промптов

**httpx + BeautifulSoup** - для парсинга веб-страниц. httpx выбрал вместо requests из-за нативной асинхронности, а BeautifulSoup остается стандартом для парсинга HTML.

**pypdf, python-docx, openpyxl** - для обработки файлов учебных планов в разных форматах (взял несколько форматов с заделом на будущее).

**Pydantic** - для валидации данных и создания строго типизированных моделей.

### Архитектурные решения

Выбрал модульную архитектуру с разделением на слои:

```
src/
├── bot/           # Логика телеграм-бота
├── services/      # Бизнес-логика
├── data/          # Модели и хранение данных
└── utils/         # Вспомогательные утилиты
```

Такая структура позволяет легко тестировать компоненты и расширять функциональность.

## Архитектура проекта

### Структура и иерархия

Проект построен по принципу разделения ответственности:

**bot/** - содержит всю логику telegram-бота:
- handlers/ - обработчики разных типов сообщений (start.py, qa.py, recommendations.py)
- keyboards/ - inline-клавиатуры для навигации
- middlewares/ - промежуточные обработчики (логирование)
- states/ - состояния FSM для диалоговых цепочек

**services/** - бизнес-логика приложения:
- parser_service.py - парсинг сайтов и файлов
- llm_service.py - взаимодействие с языковой моделью
- recommendation_service.py - генерация рекомендаций
- cache_service.py - кэширование данных

**data/** - работа с данными:
- models.py - Pydantic модели (Program, Course, UserProfile)
- json_storage.py - простое файловое хранилище

### Поток данных

1. При запуске бот парсит программы (учебные планы) и сохраняет в JSON
2. Пользователь создает профиль через диалог
3. На основе профиля и отпарсеных данных формируется контекст для LLM
4. Генерируются персональные рекомендации/ответ на вопрос

## Парсинг файлов и данных

### Многоуровневая стратегия парсинга

Столкнулся с проблемой - сайты ИТМО используют динамическую загрузку, и прямой парсинг HTML не всегда дает результат. Реализовал многоуровневый подход:

**Уровень 1:** Поиск в JSON данных страницы (Next.js __NEXT_DATA__)
Многие современные сайты хранят данные в JSON прямо в HTML. Ищу поле academic_plan в этих данных.

**Уровень 2:** Анализ JavaScript кода
Если JSON не содержит ссылок, анализирую встроенный JavaScript на предмет URL файлов с учебными планами.

**Уровень 3:** Традиционный парсинг HTML
Ищу ссылки по тексту ("Скачать учебный план") и проверяю расширения файлов.

**Fallback:** Mock данные
Если парсинг не дает результатов, использую заранее подготовленные демо-данные для демонстрации функциональности.

### Обработка разных форматов файлов

Учебные планы могут быть в PDF, DOCX или XLSX. Для каждого формата реализовал свой парсер:

**PDF:** Использую pypdf для извлечения текста, затем ищу паттерны курсов с помощью регулярных выражений.

**DOCX:** Парсю и текст параграфов, и таблицы. Часто структура более четкая, чем в PDF.

**XLSX:** Самый структурированный формат. Ищу заголовки колонок и извлекаю данные по строкам.

### Извлечение информации о курсах

Основная сложность - разнообразие форматов записи курсов. Использую несколько паттернов:
- Семестр + Название + Кредиты + Часы
- Название + Кредиты + Часы

Фильтрую нереалистичные значения и валидирую названия курсов.

## Формирование контекста для LLM

### Персонализация контекста

Контекст для LLM строю в зависимости от типа запроса:

**Для рекомендаций:**
- Профиль пользователя (образование, интересы, цели)
- Полное описание программ с курсами
- Структурированное описание выборочных дисциплин

**Для ответов на вопросы:**
- Краткий контекст с релевантными данными
- Ограничение размера для быстрой работы (15000 символов)
- Фильтрация курсов по ключевым словам из вопроса

### Оптимизация производительности

LLM может работать медленно с большими контекстами. Применяю несколько стратегий:

**Сжатие контекста:** Для Q&A режима ограничиваю размер контекста и отсекаю лишнее.

**Многоуровневый поиск:** Сначала пытаюсь найти ответ в данных напрямую, укарачивая получаемый контекст данных и только потом обращаюсь к LLM.

**Кэширование:** Результаты парсинга данных в json-ах кэширую на несколько часов.

## Система рекомендаций

### Алгоритм подбора программ

Для генерации рекомендаций анализирую:
1. Ключевые слова в интересах пользователя
2. Соответствие выборочных дисциплин интересам  
3. Опыт и цели пользователя

Если LLM-ка недоступна, использую fallback-логику - простое сопоставление ключевых слов.

# Примечание: 
Рекомендательная система реализована через локальную LLM, но эту задачу также можно было реализовать в виде теста, который проходит юзер и на основании него выдается результат на какое направление лучше пойти, но так как временные рамки накладывают временное ограничение, было принято решение остановиться на LLM.

### Персонализация выборочных дисциплин

Для каждого пользователя фильтрую выборочные курсы:
- Ищу пересечения между интересами и названиями курсов
- Учитываю семестр и сложность
- Ограничиваю количество рекомендаций 3-5 курсами

## Диалоговая система

### FSM для управления состояниями

Использую конечный автомат состояний aiogram для управления диалогами:

**COLLECTING_BACKGROUND** → **COLLECTING_INTERESTS** → **COLLECTING_GOALS** → **MAIN_MENU**

Каждое состояние обрабатывает свой тип ввода и переводит в следующее.

### Ответов на вопрос режим с фильтрацией

Реализовал систему фильтрации вопросов. Проверяю релевантность по запрещенным ключевым словам (погода, политика, игры).

Если вопрос релевантный, применяю трехуровневую стратегию ответа:
1. LLM с полным контекстом
2. Поиск в данных напрямую
3. Общий LLM без контекста
4. Fallback-ответ

### Обработка ошибок

Во всех критических местах добавил обработку исключений с логированием. Если что-то не работает, пользователь получает понятное сообщение, а не технические ошибки.

## Основные технические решения

### Асинхронная архитектура

Все операции ввода-вывода (парсинг, LLM, файловые операции) сделал асинхронными. Это критично для telegram-бота, который должен обрабатывать множество пользователей.

### Логирование и мониторинг

Использую structlog для структурированного логирования. Это помогает отслеживать работу парсера, производительность LLM и поведение пользователей.

### Конфигурация через переменные окружения

Все настройки (токены, URL модели, пути) выношу в .env файл через python-decouple. Это упрощает развертывание в разных средах.

### Graceful degradation

Система работает даже при отказе отдельных компонентов:
- Нет Ollama - показываем fallback-ответы
- Не получилось спарсить - используем mock-данные  
- LLM не отвечает - применяем простую логику поиска

## Решение основных задач

### 1. Парсинг данных с сайтов

Реализовал автоматический парсинг при запуске бота. Система ищет учебные планы на страницах программ, скачивает файлы и извлекает структурированные данные о курсах.

### 2. Диалоговая система

Создал интуитивную навигацию через inline-клавиатуры. Пользователь проходит через создание профиля, затем может задавать вопросы или получать рекомендации.

### 3. Персональные рекомендации

На основе профиля пользователя бот рекомендует подходящую программу и выборочные дисциплины. Использую как LLM для детального анализа, так и правила для базовой логики.

### 4. Фильтрация релевантных вопросов

Бот отвечает только на вопросы об обучении. Нерелевантные вопросы отфильтровываются, а также стоп-слова для промпт-инжиниринга, или пользователю предлагается переформулировать запрос.

## Тестирование системы

### Трехуровневая стратегия тестирования

Для обеспечения качества кода была реализована комплексная система тестирования с покрытием 70%+:

**Unit тесты** - изолированное тестирование компонентов:
- `test_models.py` - валидация Pydantic моделей
- `test_config.py` - конфигурация и переменные окружения  
- `test_json_storage.py` - файловое хранилище данных
- `test_cache_service.py` - кэширование с mock Redis
- `test_llm_service.py` - сервис LLM с заглушками Ollama
- `test_parser_service.py` - парсинг файлов разных форматов
- `test_recommendation_service.py` - логика рекомендаций
- `test_qa_handler.py` - обработка вопросов пользователей

**Integration тесты** - взаимодействие с внешними сервисами:
- `test_ollama_integration.py` - интеграция с Ollama API
- `test_data_integration.py` - полный цикл парсинга и сохранения данных

**E2E тесты** - сквозные сценарии:
- `test_bot_scenarios.py` - полные пользовательские сценарии в Telegram

### Инструменты тестирования

**pytest + pytest-asyncio** - основной фреймворк с поддержкой асинхронности

**pytest-cov** - измерение покрытия кода с отчетами в HTML и терминале

**pytest-mock + factory-boy** - создание моков и тестовых данных

**Маркеры тестов:**
- `@pytest.mark.integration` - тесты требующие внешних сервисов
- `@pytest.mark.e2e` - end-to-end сценарии

### Конфигурация и запуск

Настройки в `pytest.ini` обеспечивают:
- Минимальное покрытие 70%
- Автоматический asyncio режим
- Детальные отчеты о пропущенных строках

Запуск всех тестов:
```bash
pytest
```

Запуск только unit тестов:
```bash  
pytest tests/unit/
```

Запуск с покрытием:
```bash
pytest --cov=src --cov-report=html
```

### Подходы к тестированию асинхронного кода

Особое внимание уделил тестированию асинхронных операций:

**Mock внешних API** - все HTTP запросы заменяются заглушками

**Изоляция побочных эффектов** - файловые операции выполняются во временных директориях

**Graceful degradation тесты** - проверка работы при отказе внешних сервисов

## Контейнеризация и развертывание

### Docker как решение для развертывания

После завершения разработки основного функционала встал вопрос удобного развертывания. Была выбрана контейнеризация с Docker по следующим причинам:

**Изоляция зависимостей:** Python приложение и Ollama работают в отдельных контейнерах, что исключает конфликты версий.

**Воспроизводимость:** Любой разработчик может запустить идентичную среду одной командой.

**Простота развертывания:** Не нужно вручную устанавливать Python, Ollama и настраивать окружение.

### Архитектура Docker Compose

Система состоит из двух основных сервисов:

**ollama:** Контейнер с локальной LLM
- Образ: `ollama/ollama:latest`
- Порт: 11434
- Persistent volume для моделей
- Health check для готовности сервиса

**ai-masters-bot:** Python приложение
- Собирается из локального Dockerfile  
- Зависит от готовности ollama сервиса
- Монтирует директории data и logs
- Автоматический перезапуск при сбоях

### Оптимизация образа

Для уменьшения размера образа применил несколько техник:

**.dockerignore:** Исключает ненужные файлы (виртуальное окружение, кэш Python, IDE файлы)

**Multi-stage не требовался:** Приложение достаточно простое, используется slim образ Python

**Минимальные системные зависимости:** Только curl для health checks

### Health checks и мониторинг

Добавил проверки работоспособности для обоих сервисов:
- Ollama: проверка API endpoint
- Bot: проверка доступности Ollama

## Выводы и результат

Получился функциональный бот, который решает поставленные задачи. Архитектура позволяет легко добавлять новые программы и расширять функциональность.

TLDR:
Первым шагом были определены функциональные требования для бота, по которым подобран стек технологий.
Так как в регламенте было разрешено использование ИИ-инструментов, был сформирован бэклог задач для агентного режима, с подробным описанием всех функций, code-snippet-ами и критериями приемки и тестирования.
Далее в несколько этапов производилась разработка бота, с тщательной ревизией кода который написал агент.
На каждом этапе производился коммит изменений, для возможности отката в случае проблем и linter ошибок и прочих проблем при использовании ИИ агентов.
Была реализована комплексная система тестирования с покрытием 70%+ включающая unit, integration и e2e тесты.
После завершения разработки проект был контейнеризован с помощью Docker Compose для упрощения развертывания.
В качестве инструментов использовалась Cursor IDE вместе с агентным режимом для ускорения работы.