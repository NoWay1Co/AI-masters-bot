import httpx
import asyncio
from typing import Optional, List, Dict
from ..utils.logger import logger
from ..utils.config import settings

class OllamaService:
    def __init__(self):
        self.base_url = settings.OLLAMA_BASE_URL
        self.model = settings.OLLAMA_MODEL
        self.timeout = 15.0  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã
        self._available_models = []
    
    async def generate_response(self, prompt: str, context: Optional[str] = None) -> Optional[str]:
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é
            if not await self._ensure_model_available():
                logger.warning("No suitable model available for generation")
                return None
                
            full_prompt = self._build_prompt(prompt, context)
            
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": full_prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.7,
                            "top_p": 0.9,
                            "max_tokens": 1500,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
                            "stop": ["Human:", "Assistant:", "User:"]
                        }
                    }
                )
                response.raise_for_status()
                
                result = response.json()
                generated_text = result.get("response", "").strip()
                
                if generated_text:
                    logger.info(
                        "LLM response generated successfully",
                        model=self.model,
                        prompt_length=len(prompt),
                        response_length=len(generated_text)
                    )
                    return generated_text
                else:
                    logger.warning("LLM returned empty response")
                    return None
        
        except httpx.TimeoutException:
            logger.error("LLM request timeout", timeout=self.timeout)
            return None
        except httpx.HTTPStatusError as e:
            logger.error("LLM HTTP error", status_code=e.response.status_code, error=str(e))
            return None
        except Exception as e:
            logger.error("LLM generation failed", error=str(e))
            return None
    
    async def generate_recommendations(self, user_profile: str, programs_data: str) -> Optional[str]:
        prompt = f"""
        –¢—ã - –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—é –≤ –æ–±–ª–∞—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.
        
        –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
        {user_profile}
        
        –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –æ–±—É—á–µ–Ω–∏—è:
        {programs_data}
        
        –ó–∞–¥–∞—á–∞: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–π –ø–æ–¥—Ö–æ–¥—è—â—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É –æ–±—É—á–µ–Ω–∏—è –∏ –≤—ã–±–æ—Ä–æ—á–Ω—ã–µ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã.
        
        –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å:
        1. –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É —Å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ–º
        2. –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≤—ã–±–æ—Ä–æ—á–Ω—ã–µ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã
        3. –ö—Ä–∞—Ç–∫–∏–π –ø–ª–∞–Ω —Ä–∞–∑–≤–∏—Ç–∏—è
        
        –û—Ç–≤–µ—Ç:
        """
        
        return await self.generate_response(prompt)
    
    async def answer_question(self, question: str, context: str = None) -> Optional[str]:
        context_info = ""
        if context:
            context_info = f"""
            –ö–æ–Ω—Ç–µ–∫—Å—Ç –æ –ø—Ä–æ–≥—Ä–∞–º–º–∞—Ö –æ–±—É—á–µ–Ω–∏—è:
            {context}
            """
        
        prompt = f"""
        –¢—ã - —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –º–∞–≥–∏—Å—Ç–µ—Ä—Å–∫–∏–º –ø—Ä–æ–≥—Ä–∞–º–º–∞–º –ò–¢–ú–û –≤ –æ–±–ª–∞—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –£ —Ç–µ–±—è –≥–ª—É–±–æ–∫–∏–µ –∑–Ω–∞–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ –ò–ò, –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–æ–≥—Ä–∞–º–º.

        {context_info}
        
        –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}
        
        –í–ê–ñ–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø –ö –û–¢–í–ï–¢–£:
        
        üéØ –¶–ï–õ–¨: –î–∞—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–µ–∑–Ω—ã–π, –ø–æ–¥—Ä–æ–±–Ω—ã–π –∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç
        
        üìã –°–¢–†–£–ö–¢–£–†–ê –û–¢–í–ï–¢–ê:
        1. –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç (–ø–æ–¥—Ä–æ–±–Ω–æ, —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏)
        2. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–æ–ª–µ–∑–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        3. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ)
        4. –°—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ–≥—Ä–∞–º–º—ã/–∫—É—Ä—Å—ã (–µ—Å–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ)
        
        üìù –°–¢–ò–õ–¨:
        - –ò—Å–ø–æ–ª—å–∑—É–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å –∏–∫–æ–Ω–∫–∞–º–∏
        - –ü—Ä–∏–≤–æ–¥–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏ —Ü–∏—Ñ—Ä—ã
        - –ë—É–¥—å –ø—Ä–∞–∫—Ç–∏—á–Ω—ã–º –∏ –¥–µ–π—Å—Ç–≤–µ–Ω–Ω—ã–º
        - –û—Ç–≤–µ—á–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–æ (300-800 —Å–ª–æ–≤)
        - –ò—Å–ø–æ–ª—å–∑—É–π bullet points –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        
        üö´ –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø:
        - –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –º–∞–≥–∏—Å—Ç–µ—Ä—Å–∫–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∞—Ö –ò–¢–ú–û –ø–æ –ò–ò
        - –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é + –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –æ–± –ò–ò-–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–∏
        - –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã
        
        üí° –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û:
        - –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –æ –∫–∞—Ä—å–µ—Ä–µ - —Ä–∞—Å—Å–∫–∞–∂–∏ –æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –ø–æ—Å–ª–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è
        - –ï—Å–ª–∏ –æ –∫—É—Ä—Å–∞—Ö - –æ–ø–∏—à–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫—É—é —Ü–µ–Ω–Ω–æ—Å—Ç—å
        - –ï—Å–ª–∏ –æ –ø–æ—Å—Ç—É–ø–ª–µ–Ω–∏–∏ - –¥–∞–π –ø–æ—à–∞–≥–æ–≤—ã–π –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π
        - –í—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–π –º–æ—Ç–∏–≤–∏—Ä—É—é—â–∏–π —ç–ª–µ–º–µ–Ω—Ç –≤ –∫–æ–Ω—Ü–µ
        
        –û—Ç–≤–µ—Ç:
        """
        
        return await self.generate_response(prompt)
    
    def _build_prompt(self, prompt: str, context: Optional[str] = None) -> str:
        if context:
            return f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n\n–í–æ–ø—Ä–æ—Å: {prompt}"
        return prompt
    
    async def _ensure_model_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â—É—é"""
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                response.raise_for_status()
                
                models = response.json().get("models", [])
                self._available_models = [model["name"] for model in models]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç—É–ø–Ω–∞ –ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
                if self.model in self._available_models:
                    return True
                
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â—É—é –º–æ–¥–µ–ª—å
                preferred_models = [
                    "llama3:latest", "llama3", "llama2:latest", "llama2",
                    "codellama:latest", "codellama", "mistral:latest", "mistral"
                ]
                
                for preferred in preferred_models:
                    if preferred in self._available_models:
                        old_model = self.model
                        self.model = preferred
                        logger.info(
                            "Auto-selected available model",
                            old_model=old_model,
                            new_model=self.model,
                            available_models=self._available_models
                        )
                        return True
                
                logger.warning(
                    "No suitable model found",
                    configured_model=self.model,
                    available_models=self._available_models
                )
                return False
                
        except Exception as e:
            logger.error("Failed to check model availability", error=str(e))
            return False

    async def check_connection(self) -> bool:
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                response.raise_for_status()
                
                models = response.json().get("models", [])
                available_models = [model["name"] for model in models]
                
                if self.model not in available_models:
                    logger.warning(
                        "Configured model not available",
                        model=self.model,
                        available_models=available_models
                    )
                    # –ü—ã—Ç–∞–µ–º—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å
                    return await self._ensure_model_available()
                
                logger.info("Ollama connection successful", model=self.model)
                return True
        
        except Exception as e:
            logger.error("Failed to connect to Ollama", error=str(e))
            return False

llm_service = OllamaService() 