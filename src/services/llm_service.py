import httpx
import asyncio
from typing import Optional, List, Dict
from ..utils.logger import logger
from ..utils.config import settings

class OllamaService:
    def __init__(self):
        self.base_url = settings.OLLAMA_BASE_URL
        self.model = settings.OLLAMA_MODEL
        self.timeout = 15.0  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã
        self._available_models = []
    
    async def generate_response(self, prompt: str, context: Optional[str] = None) -> Optional[str]:
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é
            if not await self._ensure_model_available():
                logger.warning("No suitable model available for generation")
                return None
                
            full_prompt = self._build_prompt(prompt, context)
            
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": full_prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.7,
                            "top_p": 0.9,
                            "max_tokens": 1500,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
                            "stop": ["Human:", "Assistant:", "User:"]
                        }
                    }
                )
                response.raise_for_status()
                
                result = response.json()
                generated_text = result.get("response", "").strip()
                
                if generated_text:
                    logger.info(
                        "LLM response generated successfully",
                        model=self.model,
                        prompt_length=len(prompt),
                        response_length=len(generated_text)
                    )
                    return generated_text
                else:
                    logger.warning("LLM returned empty response")
                    return None
        
        except httpx.TimeoutException:
            logger.error("LLM request timeout", timeout=self.timeout)
            return None
        except httpx.HTTPStatusError as e:
            logger.error("LLM HTTP error", status_code=e.response.status_code, error=str(e))
            return None
        except Exception as e:
            logger.error("LLM generation failed", error=str(e))
            return None
    
    async def generate_recommendations(self, user_profile: str, programs_data: str) -> Optional[str]:
        prompt = f"""
        –¢—ã - –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—é –≤ –æ–±–ª–∞—Å—Ç–∏ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.
        
        –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
        {user_profile}
        
        –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –æ–±—É—á–µ–Ω–∏—è:
        {programs_data}
        
        –ó–∞–¥–∞—á–∞: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–π –ø–æ–¥—Ö–æ–¥—è—â—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É –æ–±—É—á–µ–Ω–∏—è –∏ –≤—ã–±–æ—Ä–æ—á–Ω—ã–µ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã.
        
        –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å:
        1. –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É —Å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ–º
        2. –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≤—ã–±–æ—Ä–æ—á–Ω—ã–µ –¥–∏—Å—Ü–∏–ø–ª–∏–Ω—ã
        3. –ö—Ä–∞—Ç–∫–∏–π –ø–ª–∞–Ω —Ä–∞–∑–≤–∏—Ç–∏—è
        
        –û—Ç–≤–µ—Ç:
        """
        
        return await self.generate_response(prompt)
    
    async def answer_question(self, question: str, context: str = None) -> Optional[str]:
        context_info = ""
        if context:
            context_info = f"""
            –ö–æ–Ω—Ç–µ–∫—Å—Ç –æ –ø—Ä–æ–≥—Ä–∞–º–º–∞—Ö –æ–±—É—á–µ–Ω–∏—è:
            {context}
            """
        
        prompt = f"""
–¢—ã –º–æ–π –¥—Ä—É–≥-–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –º–∞–≥–∏—Å—Ç–µ—Ä—Å–∫–∏–º –ø—Ä–æ–≥—Ä–∞–º–º–∞–º –ò–¢–ú–û –ø–æ –ò–ò. –û—Ç–≤–µ—á–∞–π —Ç–µ–ø–ª–æ –∏ –¥—Ä—É–∂–µ–ª—é–±–Ω–æ, –∫–∞–∫ –±—É–¥—Ç–æ –æ–±—â–∞–µ–º—Å—è –ª–∏—á–Ω–æ.

{context_info}

–í–æ–ø—Ä–æ—Å –¥—Ä—É–≥–∞: {question}

–í–ê–ñ–ù–û:
üéØ –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ–≥—Ä–∞–º–º–∞—Ö
üéØ –ù–ï –¥–æ–±–∞–≤–ª—è–π –æ–±—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é - —Ç–æ–ª—å–∫–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
üéØ –ì–æ–≤–æ—Ä–∏ –ø—Ä–æ—Å—Ç—ã–º, –¥—Ä—É–∂–µ—Å–∫–∏–º —Ç–æ–Ω–æ–º
üéØ –û–±—Ä–∞—â–∞–π—Å—è –Ω–∞ "—Ç—ã" –∏ –∏—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ –¥–ª—è —Ç–µ–ø–ª–æ—Ç—ã

–°–¢–†–£–ö–¢–£–†–ê –û–¢–í–ï–¢–ê:
1. –ü—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
2. –ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ)
3. –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

–°–¢–ò–õ–¨: –î—Ä—É–∂–µ—Å–∫–∏–π, —Ç–µ–ø–ª—ã–π, –∫–∞–∫ —Ä–∞–∑–≥–æ–≤–æ—Ä —Å –ø—Ä–∏—è—Ç–µ–ª–µ–º. –ò–∑–±–µ–≥–∞–π —Ñ–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏.

–û—Ç–≤–µ—Ç:
"""
        
        return await self.generate_response(prompt)
    
    def _build_prompt(self, prompt: str, context: Optional[str] = None) -> str:
        if context:
            return f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}\n\n–í–æ–ø—Ä–æ—Å: {prompt}"
        return prompt
    
    async def _ensure_model_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â—É—é"""
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                response.raise_for_status()
                
                models = response.json().get("models", [])
                self._available_models = [model["name"] for model in models]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç—É–ø–Ω–∞ –ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
                if self.model in self._available_models:
                    return True
                
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â—É—é –º–æ–¥–µ–ª—å
                preferred_models = [
                    "llama3:latest", "llama3", "llama2:latest", "llama2",
                    "codellama:latest", "codellama", "mistral:latest", "mistral"
                ]
                
                for preferred in preferred_models:
                    if preferred in self._available_models:
                        old_model = self.model
                        self.model = preferred
                        logger.info(
                            "Auto-selected available model",
                            old_model=old_model,
                            new_model=self.model,
                            available_models=self._available_models
                        )
                        return True
                
                logger.warning(
                    "No suitable model found",
                    configured_model=self.model,
                    available_models=self._available_models
                )
                return False
                
        except Exception as e:
            logger.error("Failed to check model availability", error=str(e))
            return False

    async def check_connection(self) -> bool:
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                response.raise_for_status()
                
                models = response.json().get("models", [])
                available_models = [model["name"] for model in models]
                
                if self.model not in available_models:
                    logger.warning(
                        "Configured model not available",
                        model=self.model,
                        available_models=available_models
                    )
                    # –ü—ã—Ç–∞–µ–º—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å
                    return await self._ensure_model_available()
                
                logger.info("Ollama connection successful", model=self.model)
                return True
        
        except Exception as e:
            logger.error("Failed to connect to Ollama", error=str(e))
            return False

llm_service = OllamaService() 